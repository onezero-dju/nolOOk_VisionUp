# ìš”ì•½->í‚¤ì›Œë“œ->ë¹ˆë²ˆí•œ í‚¤ì›Œë“œ ë„ì¶œ ì½”ë“œ (2): ëª¨ë¸ ë°”ê¾¸ê³  ìš”ì•½ ì„±ëŠ¥ UP - Fail

import streamlit as st
from fastapi import FastAPI, UploadFile, File
from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM
from collections import Counter
import os
import re

app = FastAPI()

# í•„ìš”í•œ ë””ë ‰í† ë¦¬ ìƒì„±
os.makedirs(".cache/files", exist_ok=True)

# ìš”ì•½ ëª¨ë¸ ì´ˆê¸°í™”
tokenizer = AutoTokenizer.from_pretrained("facebook/bart-large-cnn")
model = AutoModelForSeq2SeqLM.from_pretrained("facebook/bart-large-cnn")
summarizer = pipeline("text-generation", model=model, tokenizer=tokenizer)

@app.post("/upload-markdown/")
async def upload_markdown(file: UploadFile = File(...)):
    content = await file.read()
    text = content.decode("utf-8")
    # íŒŒì¼ ì €ì¥
    file_path = f"./.cache/files/{file.filename}"
    with open(file_path, "w") as f:
        f.write(text)
    # íŒŒì¼ ë‚´ìš© ìš”ì•½
    summary = summarizer(text, max_length=1024, min_length=100, do_sample=False)[0]['summary_text']
    return {"filename": file.filename, "summary": summary, "message": "File uploaded and summarized successfully"}

st.set_page_config(page_title="Markdown íŒŒì¼ ìš”ì•½ ë° í‚¤ì›Œë“œ ì¶”ì¶œ", page_icon="ğŸ“„")
st.title("Markdown íŒŒì¼ ìš”ì•½ ë° í‚¤ì›Œë“œ ì¶”ì¶œ")

# í‚¤ì›Œë“œ ì¶”ì¶œ í•¨ìˆ˜
def extract_keywords(text, num_keywords=5):
    words = re.findall(r'\b\w+\b', text.lower())
    count = Counter(words)
    keywords = count.most_common(num_keywords)
    return [word for word, freq in keywords]

uploaded_file = st.file_uploader("Markdown íŒŒì¼ ì—…ë¡œë“œ", type=["md"])
if uploaded_file:
    content = uploaded_file.getvalue().decode("utf-8")
    
    # íŒŒì¼ ë‚´ìš© ê¸°ë°˜ìœ¼ë¡œ ê°„ë‹¨í•˜ê²Œ ìš”ì•½í•´ì¤˜
    prompt_text = f"íŒŒì¼ ë‚´ìš© ê¸°ë°˜ìœ¼ë¡œ ê°„ë‹¨í•˜ê²Œ ìš”ì•½í•´ì¤˜:\n{content}"
    summary_results = summarizer(prompt_text, max_length=150, num_return_sequences=1)
    summary_text = summary_results[0]['generated_text']

    st.subheader("ìš”ì•½ëœ ë‚´ìš©:")
    st.write(summary_text)

    # í‚¤ì›Œë“œ ì¶”ì¶œ
    keywords = extract_keywords(summary_text)
    st.subheader("ì¶”ì¶œëœ í‚¤ì›Œë“œ:")
    st.write([word for word, _ in keywords])
    st.subheader("ê°€ì¥ ë§ì´ ë‚˜ì˜¨ í‚¤ì›Œë“œ:")
    st.write(keywords[0][0] if keywords else "No keywords found")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)